31장 - ConvNext_v2_large의 경우 gpu memory limit으로 인하여 batch를 8로 주어서 학습하였습니다. 같이 epoch를 10으로 주었지만, 성능은 base에 비해서 떨어집니다.
32장 - 모델 크기가 커질 수록, 성능이 좋아지는 것처럼 보이지만, 실제로 성능은 large가 base보다 떨어집니다.
35장 - nano와 base간의 loss, accuracy차이가 크게 줄어든다.
37장 - 4way와 6way간의 성능차이는 train데이터만 활용해 가지고는 거의 나지 않는다.
40장 - ConvNext_v2_base로 4-way데이터 증강에 20epoch주고 학습한 모델이, convnext_v2_nano에  4-way데이터 증강 주고 10epoch만 준 모델에 비해서 성능이 오히려 떨어진다. 그만큼 추가로 2000장의 데이터를 활용하는 것이 도움이 크다. 또한 early-stopping을 사용하지 않기에 중간에 모델을 저장하지 않고, 그렇기 때문에 gpu memory를 훨씬 더 많이 batch에 할당할 수 있다.
43장 - atto에 exponentialLR을 사용한것과 사용하지 않는 것을 비교해 보면, 사용한 쪽이 성능이 약간 더 높게 나오는 것을 알 수 있다. 이 증가 폭은 batch size를 늘리는 것보다 더 많이 늘어난다.
전에 소개한 결과를 보면 nano 4way, train data only기준으로, 10에포크에 0.9012, 20에포크에 0.9052가 나온다. 즉 10에포크 이후로는 모델의 성능 향상이 잘 이뤄지지 않는다. 그렇기에 10에포크 이후에도 9.0e-5보다 낮은 lr를 유지하며, 점점 더 낮은 lr로 학습해 나가면 더 미세하게, 가중치 조정을 할 수 있을 것이다. 실제로 exponential scheduler기준 gamma를 0.9로 준 상태에서 1.5e-4 lr로 시작하면, (1.5e-04 -> 8.85e-05(5에포크) -> 5.23e-05(10에포크) -> 2.02e-05(20에포크)) lr이 변화한다. 즉 4에포크 까지는 9.0e-5에포크보다 높게 학습률을 적용해 학습하고, 그 이후 에포크 8까지 7.0e-5에포크보다 높게 학습률을 적용한다. 그 이후로는 학습률 감소율이 점차 줄어 들면서 비슷한 낮은 학습률을 유지하며 학습하게 된다.
44장 - 그래프를 자세히 보면, stepLR은 살짝 더 train data에 모델을 최적화 시키는데 성능이 떨어짐을 알 수 있다.
45장 - scheduler를 사용한 편이 train data의 loss와 acc면에서 더 좋다.
46장 - nano보다 크면서 base보다 작은 tiny가 colab gpu(v100) 활용시간 까지 고려하였을때, 가장 높은 모델 score확보에 적합하였다.

